{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     -------------------------------------- 62.6/62.6 kB 562.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vskon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.2)\n",
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 503.3 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "     -------------------------------------- 96.6/96.6 kB 918.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vskon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vskon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vskon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vskon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Collecting lxml>=2.3.2\n",
      "  Downloading lxml-4.9.3-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 925.0 kB/s eta 0:00:00\n",
      "Installing collected packages: lxml, charset-normalizer, requests, python-docx\n",
      "  Running setup.py install for python-docx: started\n",
      "  Running setup.py install for python-docx: finished with status 'done'\n",
      "Successfully installed charset-normalizer-3.2.0 lxml-4.9.3 python-docx-0.8.11 requests-2.31.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl\n",
      "  DEPRECATION: python-docx is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to Extract Questions and their options\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "\n",
    "# base_url = 'https://www.indiabix.com/aptitude/time-and-work/{}'\n",
    "\n",
    "base_url = 'https://www.indiabix.com/aptitude/profit-and-loss/{}'\n",
    "page_numbers = range(1, 30)\n",
    "\n",
    "document = Document()\n",
    "\n",
    "for page_number in page_numbers:\n",
    "    if page_number == 1:\n",
    "        url = base_url.format('')\n",
    "    elif page_number < 10:\n",
    "        url = base_url.format(f'01800{page_number}')\n",
    "    else:\n",
    "        url = base_url.format(f'0180{page_number}')\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    questions = soup.find_all('div', class_='bix-div-container')\n",
    "    for question in questions:\n",
    "        q_no_elem = question.find('div', class_='bix-td-qno')\n",
    "        q_text_elem = question.find('div', class_='bix-td-qtxt')\n",
    "        if q_no_elem and q_text_elem:\n",
    "            q_no = q_no_elem.text.strip()\n",
    "            q_text = q_text_elem.text.strip()\n",
    "            for sup in q_text_elem.find_all('sup'):\n",
    "                sup_text = sup.text.strip()\n",
    "                sup.replace_with(f\"^{sup_text}\")\n",
    "            p = document.add_paragraph(f\"{q_no} {q_text}\")\n",
    "        \n",
    "        options = question.find_all('div', class_='bix-td-option-val')\n",
    "        option_letters = ['A', 'B', 'C', 'D', 'E']\n",
    "        p = document.add_paragraph()\n",
    "        for i, option in enumerate(options):\n",
    "            if i < len(option_letters):\n",
    "                option_no = option_letters[i]\n",
    "                option_text = option.text.strip()\n",
    "                for sup in option.find_all('sup'):\n",
    "                    sup_text = sup.text.strip()\n",
    "                    sup.replace_with(f\"^{sup_text}\")\n",
    "                p.add_run(f\"{option_no}) {option_text}\\t\")\n",
    "            else:\n",
    "                p.add_run(f\"{option.text.strip()}\\t\")\n",
    "\n",
    "document.save('questions.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to extract question number and their correct options\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "\n",
    "base_url = 'https://www.indiabix.com/aptitude/profit-and-loss/{}'\n",
    "page_numbers = range(1, 30)\n",
    "\n",
    "document = Document()\n",
    "\n",
    "for page_number in page_numbers:\n",
    "    if page_number == 1:\n",
    "        url = base_url.format('')\n",
    "    elif page_number < 10:\n",
    "        url = base_url.format(f'01800{page_number}')\n",
    "    else:\n",
    "        url = base_url.format(f'0180{page_number}')\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    questions = soup.find_all('div', class_='bix-div-container')\n",
    "    for question in questions:\n",
    "        q_no_elem = question.find('div', class_='bix-td-qno')\n",
    "        answer_elem = question.find('input', class_='jq-hdnakq')\n",
    "        if q_no_elem and answer_elem:\n",
    "            q_no = q_no_elem.text.strip()\n",
    "            answer = answer_elem['value']\n",
    "            document.add_paragraph(f\"{q_no} {answer}\")\n",
    "\n",
    "document.save('answers.docx')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
